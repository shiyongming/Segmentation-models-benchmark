{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74bd8c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3615dd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdoel_index = 0\n",
    "\n",
    "model_list = ['fcn_resnet50', #0\n",
    "              'fcn_resnet101',#1\n",
    "              'deeplabv3_resnet50', #2\n",
    "              'deeplabv3_resnet101', #3 \n",
    "              'deeplabv3_mobilenet_v3_large', #4\n",
    "              'lraspp_mobilenet_v3_large' #5\n",
    "             ]\n",
    "\n",
    "def fcn_resnet50():\n",
    "    model = torchvision.models.segmentation.fcn_resnet50(pretrained=True).cuda()\n",
    "    return model\n",
    "\n",
    "def fcn_resnet101():\n",
    "    model = torchvision.models.segmentation.fcn_resnet101(pretrained=True).cuda()\n",
    "    return model\n",
    "\n",
    "def deeplabv3_resnet50():\n",
    "    model = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True).cuda()\n",
    "    return model\n",
    "\n",
    "def deeplabv3_resnet101():\n",
    "    model = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=True).cuda()\n",
    "    return model\n",
    "\n",
    "def deeplabv3_mobilenet_v3_large():\n",
    "    model = torchvision.models.segmentation.deeplabv3_mobilenet_v3_large(pretrained=True).cuda()\n",
    "    return model\n",
    "\n",
    "def lraspp_mobilenet_v3_large():\n",
    "    model = torchvision.models.segmentation.lraspp_mobilenet_v3_large(pretrained=True).cuda()\n",
    "    return model\n",
    "\n",
    "def default():\n",
    "    print('Please choose the right model index')\n",
    "    print(model_list)\n",
    "    return None\n",
    "choose_model = {'fcn_resnet50': fcn_resnet50,\n",
    "                'fcn_resnet101': fcn_resnet101,\n",
    "                'deeplabv3_resnet50': deeplabv3_resnet50,\n",
    "                'deeplabv3_resnet101': deeplabv3_resnet101,\n",
    "                'deeplabv3_mobilenet_v3_large': deeplabv3_mobilenet_v3_large,\n",
    "                'lraspp_mobilenet_v3_large': lraspp_mobilenet_v3_large\n",
    "               }\n",
    "\n",
    "model_name = model_list[mdoel_index]\n",
    "model = choose_model.get(model_name, default)()\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "238af456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 21, 640, 512]) -6.3515849113464355 14.941158294677734\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms.functional import convert_image_dtype\n",
    "from pathlib import Path\n",
    "\n",
    "input_image1 = read_image(str(Path('assets') / 'dog1.jpg'))\n",
    "# input_image2 = read_image(str(Path('assets') / 'dog2.jpg'))\n",
    "model = model.eval()\n",
    "\n",
    "batch_images = torch.stack([input_image1, input_image1])\n",
    "batch = convert_image_dtype(batch_images, dtype=torch.float)\n",
    "\n",
    "normalized_batch = F.normalize(batch, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "\n",
    "output = model(normalized_batch.cuda())['out']\n",
    "print(output.shape, output.min().item(), output.max().item())\n",
    "\n",
    "sem_classes = [\n",
    "    '__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
    "    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n",
    "]\n",
    "sem_class_to_idx = {cls: idx for (idx, cls) in enumerate(sem_classes)}\n",
    "\n",
    "normalized_masks = torch.nn.functional.softmax(output, dim=1)\n",
    "\n",
    "def show(imgs):\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddecd5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog1_masks shape = torch.Size([21, 640, 512]), dtype = torch.float32\n",
      "dog1_all_classes_masks = torch.Size([21, 640, 512]), dtype = torch.bool\n",
      "-------plot the multi boolen masks on orignal images-------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torchvision.utils import draw_segmentation_masks\n",
    "num_classes = normalized_masks.shape[1]\n",
    "dog1_masks = normalized_masks[0]\n",
    "class_dim = 0\n",
    "dog1_all_classes_masks = dog1_masks.argmax(class_dim).cuda() == torch.arange(num_classes)[:, None, None].cuda()\n",
    "\n",
    "print(f\"dog1_masks shape = {dog1_masks.shape}, dtype = {dog1_masks.dtype}\")\n",
    "print(f\"dog1_all_classes_masks = {dog1_all_classes_masks.shape}, dtype = {dog1_all_classes_masks.dtype}\")\n",
    "print('-------plot the multi boolen masks on orignal images-------')\n",
    "dog_with_all_masks = draw_segmentation_masks(input_image1, masks=dog1_all_classes_masks, alpha=.6)\n",
    "show(dog_with_all_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b6751b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "color_channel = 3\n",
    "height = 256\n",
    "width = 256\n",
    "\n",
    "input_names = ['input0'] + ['learned_%d' %i for i in range(16)]\n",
    "output_names = ['dense_out']\n",
    "dummy_input = torch.randn(batch_size, color_channel, width, height, device='cuda')\n",
    "onnx_model_name = model_name + '.onnx'\n",
    "dynamic_axes = {'input0': {0:'batch', 2:'height', 3:'width'}}\n",
    "\n",
    "torch.onnx.export(model,\n",
    "                 dummy_input,\n",
    "                 onnx_model_name,\n",
    "                 input_names=input_names,\n",
    "                 output_names=output_names,\n",
    "                 opset_version=12,\n",
    "                 dynamic_axes=dynamic_axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4210ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "from calibrator import EntropyCalibrator\n",
    "import common\n",
    "calibration_cache = \"mnist_calibration.cache\"\n",
    "\n",
    "\n",
    "def build_int8_engine(onnx_file_path, calibrator, batch_size, calibration_cache):\n",
    "    # with trt.Builder(TRT_LOGGER) as builder, builder.create_network() as network, builder.create_builder_config() as config, trt.CaffeParser() as parser:\n",
    "    EXPLICIT_BATCH = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "    with trt.Builder(TRT_LOGGER) as builder, builder.create_network(common.EXPLICIT_BATCH) as network, \\\n",
    "            builder.create_builder_config() as config, trt.OnnxParser(network,TRT_LOGGER) as parser:\n",
    "        # We set the builder batch size to be the same as the calibrator's, as we use the same batches\n",
    "        # during inference. Note that this is not required in general, and inference batch size is\n",
    "        # independent of calibration batch size.\n",
    "        builder.max_batch_size = batch_size\n",
    "\n",
    "        config.max_workspace_size = common.GiB(1)\n",
    "        config.set_flag(trt.BuilderFlag.INT8)\n",
    "        config.set_flag(trt.BuilderFlag.STRICT_TYPES)\n",
    "        config.int8_calibrator = calib\n",
    "\n",
    "        # Parse Onnx model\n",
    "        with open(onnx_file_path, 'rb') as model:\n",
    "            print('Beginning ONNX file parsing')\n",
    "            if not parser.parse(model.read()):\n",
    "                print('ERROR: Failed to parse the ONNX file.')\n",
    "                for error in range(parser.num_errors):\n",
    "                    print(parser.get_error(error))\n",
    "                return None\n",
    "        \n",
    "        # For the fixed batch, please use the following code\n",
    "        #network.get_input(0).shape = [batch_size, 3, 32, 32]\n",
    "        \n",
    "        # For dynamic batch, please use the following code\n",
    "        profile = builder.create_optimization_profile();\n",
    "        profile.set_shape(\"input0\", (1, 3, 256, 256), (32, 3, 512, 512), (64, 3, 640, 640))\n",
    "        config.add_optimization_profile(profile)\n",
    "        config.set_calibration_profile(profile)\n",
    "\n",
    "#         #Decide which layers fallback to FP32. #If all layers fallback to FP32, you can use 'index>-1'\n",
    "#         for index, layer in enumerate(network):\n",
    "#             print('layer index', index, ':', layer.type, layer.name)\n",
    "#             if index < 10:\n",
    "#                 if layer.type == trt.LayerType.ACTIVATION or \\\n",
    "#                         layer.type == trt.LayerType.CONVOLUTION or \\\n",
    "#                         layer.type == trt.LayerType.FULLY_CONNECTED or \\\n",
    "#                         layer.type == trt.LayerType.SCALE:\n",
    "#                     print('layer index', index, ':', layer.type, 'will be', 'fallback to fp32!')\n",
    "#                     layer.precision = trt.float32\n",
    "#                     layer.set_output_type(0, trt.float32)\n",
    "                    \n",
    "#         ### setting dynamic range for the output of activation layer\n",
    "#         ### here, we set the output of layer[64] ReLu to [-6, 6]\n",
    "#         layer=network[45]\n",
    "#         tensor = layer.get_output(0)\n",
    "#         tensor.dynamic_range = (-6.0, 6.0)  \n",
    "\n",
    "        # Start to build engine and do int8 calibration.\n",
    "        print('--- Starting to build engine! ---')\n",
    "        engine = builder.build_engine(network, config)\n",
    "        print('--- Building engine is finished! ---')\n",
    "        \n",
    "        ### Using the calibration cache to pick out correspondding network layer\n",
    "        cache_dict = convert_calib_cache(calibration_cache)\n",
    "        if cache_dict is not None:\n",
    "            for index, layer in enumerate(network):\n",
    "                for i in range(layer.num_outputs):\n",
    "                    output_tensor = layer.get_output(i)\n",
    "                    if output_tensor.name in cache_dict:\n",
    "                        hex_str = cache_dict[output_tensor.name]                    \n",
    "                        scale = struct.unpack('!f', bytes.fromhex(hex_str))[0]\n",
    "                        print('Layer index is:', index, '; ', \\\n",
    "                              'Activations dynamic range is: (-/+)', scale * 127.0, '; ',\\\n",
    "                              'Layer type is:', layer.type)#, ';', \\\n",
    "                              #'output tensor name is:', output_tensor.name)\n",
    "        return engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed3147e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/11/2022-07:43:36] [TRT] [I] [MemUsageChange] Init CUDA: CPU +448, GPU +0, now: CPU 3913, GPU 6058 (MiB)\n",
      "Beginning ONNX file parsing\n",
      "[01/11/2022-07:43:36] [TRT] [I] [MemUsageSnapshot] Begin constructing builder kernel library: CPU 3913 MiB, GPU 6058 MiB\n",
      "[01/11/2022-07:43:36] [TRT] [I] [MemUsageSnapshot] End constructing builder kernel library: CPU 4067 MiB, GPU 6102 MiB\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::BatchTilePlugin_TRT version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::BatchedNMS_TRT version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::BatchedNMSDynamic_TRT version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::CoordConvAC version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::CropAndResize version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::CropAndResizeDynamic version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::DetectionLayer_TRT version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::EfficientNMS_TRT version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::EfficientNMS_ONNX_TRT version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::EfficientNMS_TFTRT_TRT version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::FlattenConcat_TRT version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::GenerateDetection_TRT version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::GridAnchor_TRT version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::GridAnchorRect_TRT version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::InstanceNormalization_TRT version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::LReLU_TRT version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::MultilevelCropAndResize_TRT version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::MultilevelProposeROI_TRT version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::NMS_TRT version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::NMSDynamic_TRT version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::Normalize_TRT version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::PriorBox_TRT version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::ProposalLayer_TRT version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::Proposal version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::ProposalDynamic version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::PyramidROIAlign_TRT version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::Region_TRT version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::Reorg_TRT version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::ResizeNearest_TRT version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::RPROI_TRT version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::ScatterND version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::SpecialSlice_TRT version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registered plugin creator - ::Split version 1\n",
      "[01/11/2022-07:43:36] [TRT] [V] Adding network input: input0 with dtype: float32, dimensions: (-1, 3, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: input0 for ONNX tensor: input0\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: classifier.4.weight\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: classifier.4.bias\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: aux_classifier.4.weight\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: aux_classifier.4.bias\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 548\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 549\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 551\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 552\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 554\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 555\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 557\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 558\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 560\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 561\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 563\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 564\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 566\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 567\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 569\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 570\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 572\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 573\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 575\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 576\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 578\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 579\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 581\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 582\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 584\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 585\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 587\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 588\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 590\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 591\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 593\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 594\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 596\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 597\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 599\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 600\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 602\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 603\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 605\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 606\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 608\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 609\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 611\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 612\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 614\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 615\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 617\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 618\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 620\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 621\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 623\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 624\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 626\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 627\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 629\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 630\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 632\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 633\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 635\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 636\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 638\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 639\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 641\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 642\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 644\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 645\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 647\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 648\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 650\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 651\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 653\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 654\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 656\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 657\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 659\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 660\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 662\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 663\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 665\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 666\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 668\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 669\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 671\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 672\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 674\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 675\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 677\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 678\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 680\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 681\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 683\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 684\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 686\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 687\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 689\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 690\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 692\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 693\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 695\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 696\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 698\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 699\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 701\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 702\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 704\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 705\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 707\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 708\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 710\n",
      "[01/11/2022-07:43:36] [TRT] [V] Importing initializer: 711\n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Shape_0 [Shape]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: input0\n",
      "[01/11/2022-07:43:36] [TRT] [V] Shape_0 [Shape] inputs: [input0 -> (-1, 3, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Shape_0 for ONNX node: Shape_0\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 335 for ONNX tensor: 335\n",
      "[01/11/2022-07:43:36] [TRT] [V] Shape_0 [Shape] outputs: [335 -> (4)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Constant_1 [Constant]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Constant_1 [Constant] inputs: \n",
      "[01/11/2022-07:43:36] [TRT] [W] parsers/onnx/onnx2trt_utils.cpp:364: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
      "[01/11/2022-07:43:36] [TRT] [V] Constant_1 [Constant] outputs: [336 -> ()[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Gather_2 [Gather]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 335\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 336\n",
      "[01/11/2022-07:43:36] [TRT] [V] Gather_2 [Gather] inputs: [335 -> (4)[INT32]], [336 -> ()[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: 336 for ONNX node: 336\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using Gather axis: 0\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Gather_2 for ONNX node: Gather_2\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 337 for ONNX tensor: 337\n",
      "[01/11/2022-07:43:36] [TRT] [V] Gather_2 [Gather] outputs: [337 -> ()[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Shape_3 [Shape]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: input0\n",
      "[01/11/2022-07:43:36] [TRT] [V] Shape_3 [Shape] inputs: [input0 -> (-1, 3, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Shape_3 for ONNX node: Shape_3\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 338 for ONNX tensor: 338\n",
      "[01/11/2022-07:43:36] [TRT] [V] Shape_3 [Shape] outputs: [338 -> (4)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Constant_4 [Constant]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Constant_4 [Constant] inputs: \n",
      "[01/11/2022-07:43:36] [TRT] [V] Constant_4 [Constant] outputs: [339 -> ()[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Gather_5 [Gather]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 338\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 339\n",
      "[01/11/2022-07:43:36] [TRT] [V] Gather_5 [Gather] inputs: [338 -> (4)[INT32]], [339 -> ()[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: 339 for ONNX node: 339\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using Gather axis: 0\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Gather_5 for ONNX node: Gather_5\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 340 for ONNX tensor: 340\n",
      "[01/11/2022-07:43:36] [TRT] [V] Gather_5 [Gather] outputs: [340 -> ()[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_6 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: input0\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 548\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 549\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_6 [Conv] inputs: [input0 -> (-1, 3, -1, -1)[FLOAT]], [548 -> (64, 3, 7, 7)[FLOAT]], [549 -> (64)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 3, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_6 for ONNX node: Conv_6\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (7, 7), strides: (2, 2), prepadding: (3, 3), postpadding: (3, 3), dilations: (1, 1), numOutputs: 64\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 64, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 547 for ONNX tensor: 547\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_6 [Conv] outputs: [547 -> (-1, 64, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_7 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 547\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_7 [Relu] inputs: [547 -> (-1, 64, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_7 for ONNX node: Relu_7\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 343 for ONNX tensor: 343\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_7 [Relu] outputs: [343 -> (-1, 64, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: MaxPool_8 [MaxPool]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 343\n",
      "[01/11/2022-07:43:36] [TRT] [V] MaxPool_8 [MaxPool] inputs: [343 -> (-1, 64, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: MaxPool_8 for ONNX node: MaxPool_8\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 344 for ONNX tensor: 344\n",
      "[01/11/2022-07:43:36] [TRT] [V] MaxPool_8 [MaxPool] outputs: [344 -> (-1, 64, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_9 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 344\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 551\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 552\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_9 [Conv] inputs: [344 -> (-1, 64, -1, -1)[FLOAT]], [551 -> (64, 64, 1, 1)[FLOAT]], [552 -> (64)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 64, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_9 for ONNX node: Conv_9\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 64\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 64, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 550 for ONNX tensor: 550\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_9 [Conv] outputs: [550 -> (-1, 64, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_10 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 550\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_10 [Relu] inputs: [550 -> (-1, 64, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_10 for ONNX node: Relu_10\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 347 for ONNX tensor: 347\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_10 [Relu] outputs: [347 -> (-1, 64, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_11 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 347\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 554\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 555\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_11 [Conv] inputs: [347 -> (-1, 64, -1, -1)[FLOAT]], [554 -> (64, 64, 3, 3)[FLOAT]], [555 -> (64)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 64, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_11 for ONNX node: Conv_11\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 64, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 553 for ONNX tensor: 553\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_11 [Conv] outputs: [553 -> (-1, 64, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_12 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 553\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_12 [Relu] inputs: [553 -> (-1, 64, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_12 for ONNX node: Relu_12\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 350 for ONNX tensor: 350\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_12 [Relu] outputs: [350 -> (-1, 64, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_13 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 350\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 557\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 558\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_13 [Conv] inputs: [350 -> (-1, 64, -1, -1)[FLOAT]], [557 -> (256, 64, 1, 1)[FLOAT]], [558 -> (256)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 64, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_13 for ONNX node: Conv_13\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 556 for ONNX tensor: 556\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_13 [Conv] outputs: [556 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_14 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 344\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 560\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 561\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_14 [Conv] inputs: [344 -> (-1, 64, -1, -1)[FLOAT]], [560 -> (256, 64, 1, 1)[FLOAT]], [561 -> (256)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 64, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_14 for ONNX node: Conv_14\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 559 for ONNX tensor: 559\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_14 [Conv] outputs: [559 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Add_15 [Add]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 556\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 559\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_15 [Add] inputs: [556 -> (-1, 256, -1, -1)[FLOAT]], [559 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Add_15 for ONNX node: Add_15\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 355 for ONNX tensor: 355\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_15 [Add] outputs: [355 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_16 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 355\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_16 [Relu] inputs: [355 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_16 for ONNX node: Relu_16\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 356 for ONNX tensor: 356\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_16 [Relu] outputs: [356 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_17 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 356\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 563\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 564\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_17 [Conv] inputs: [356 -> (-1, 256, -1, -1)[FLOAT]], [563 -> (64, 256, 1, 1)[FLOAT]], [564 -> (64)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_17 for ONNX node: Conv_17\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 64\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 64, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 562 for ONNX tensor: 562\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_17 [Conv] outputs: [562 -> (-1, 64, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_18 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 562\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_18 [Relu] inputs: [562 -> (-1, 64, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_18 for ONNX node: Relu_18\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 359 for ONNX tensor: 359\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_18 [Relu] outputs: [359 -> (-1, 64, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_19 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 359\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 566\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 567\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_19 [Conv] inputs: [359 -> (-1, 64, -1, -1)[FLOAT]], [566 -> (64, 64, 3, 3)[FLOAT]], [567 -> (64)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 64, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_19 for ONNX node: Conv_19\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 64, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 565 for ONNX tensor: 565\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_19 [Conv] outputs: [565 -> (-1, 64, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_20 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 565\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_20 [Relu] inputs: [565 -> (-1, 64, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_20 for ONNX node: Relu_20\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 362 for ONNX tensor: 362\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_20 [Relu] outputs: [362 -> (-1, 64, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_21 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 362\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 569\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 570\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_21 [Conv] inputs: [362 -> (-1, 64, -1, -1)[FLOAT]], [569 -> (256, 64, 1, 1)[FLOAT]], [570 -> (256)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 64, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_21 for ONNX node: Conv_21\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 568 for ONNX tensor: 568\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_21 [Conv] outputs: [568 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Add_22 [Add]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 568\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 356\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_22 [Add] inputs: [568 -> (-1, 256, -1, -1)[FLOAT]], [356 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Add_22 for ONNX node: Add_22\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 365 for ONNX tensor: 365\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_22 [Add] outputs: [365 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_23 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 365\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_23 [Relu] inputs: [365 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_23 for ONNX node: Relu_23\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 366 for ONNX tensor: 366\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_23 [Relu] outputs: [366 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_24 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 366\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 572\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 573\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_24 [Conv] inputs: [366 -> (-1, 256, -1, -1)[FLOAT]], [572 -> (64, 256, 1, 1)[FLOAT]], [573 -> (64)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_24 for ONNX node: Conv_24\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 64\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 64, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 571 for ONNX tensor: 571\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_24 [Conv] outputs: [571 -> (-1, 64, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_25 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 571\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_25 [Relu] inputs: [571 -> (-1, 64, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_25 for ONNX node: Relu_25\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 369 for ONNX tensor: 369\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_25 [Relu] outputs: [369 -> (-1, 64, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_26 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 369\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 575\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 576\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_26 [Conv] inputs: [369 -> (-1, 64, -1, -1)[FLOAT]], [575 -> (64, 64, 3, 3)[FLOAT]], [576 -> (64)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 64, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_26 for ONNX node: Conv_26\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 64\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 64, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 574 for ONNX tensor: 574\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_26 [Conv] outputs: [574 -> (-1, 64, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_27 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 574\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_27 [Relu] inputs: [574 -> (-1, 64, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_27 for ONNX node: Relu_27\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 372 for ONNX tensor: 372\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_27 [Relu] outputs: [372 -> (-1, 64, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_28 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 372\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 578\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 579\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_28 [Conv] inputs: [372 -> (-1, 64, -1, -1)[FLOAT]], [578 -> (256, 64, 1, 1)[FLOAT]], [579 -> (256)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 64, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_28 for ONNX node: Conv_28\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 577 for ONNX tensor: 577\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_28 [Conv] outputs: [577 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Add_29 [Add]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 577\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 366\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_29 [Add] inputs: [577 -> (-1, 256, -1, -1)[FLOAT]], [366 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Add_29 for ONNX node: Add_29\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 375 for ONNX tensor: 375\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_29 [Add] outputs: [375 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_30 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 375\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_30 [Relu] inputs: [375 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_30 for ONNX node: Relu_30\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 376 for ONNX tensor: 376\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_30 [Relu] outputs: [376 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_31 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 376\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 581\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 582\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_31 [Conv] inputs: [376 -> (-1, 256, -1, -1)[FLOAT]], [581 -> (128, 256, 1, 1)[FLOAT]], [582 -> (128)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_31 for ONNX node: Conv_31\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 128\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 128, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 580 for ONNX tensor: 580\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_31 [Conv] outputs: [580 -> (-1, 128, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_32 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 580\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_32 [Relu] inputs: [580 -> (-1, 128, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_32 for ONNX node: Relu_32\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 379 for ONNX tensor: 379\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_32 [Relu] outputs: [379 -> (-1, 128, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_33 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 379\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 584\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 585\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_33 [Conv] inputs: [379 -> (-1, 128, -1, -1)[FLOAT]], [584 -> (128, 128, 3, 3)[FLOAT]], [585 -> (128)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 128, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_33 for ONNX node: Conv_33\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (3, 3), strides: (2, 2), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 128\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 128, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 583 for ONNX tensor: 583\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_33 [Conv] outputs: [583 -> (-1, 128, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_34 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 583\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_34 [Relu] inputs: [583 -> (-1, 128, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_34 for ONNX node: Relu_34\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 382 for ONNX tensor: 382\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_34 [Relu] outputs: [382 -> (-1, 128, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_35 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 382\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 587\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 588\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_35 [Conv] inputs: [382 -> (-1, 128, -1, -1)[FLOAT]], [587 -> (512, 128, 1, 1)[FLOAT]], [588 -> (512)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 128, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_35 for ONNX node: Conv_35\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 512\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 512, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 586 for ONNX tensor: 586\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_35 [Conv] outputs: [586 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_36 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 376\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 590\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 591\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_36 [Conv] inputs: [376 -> (-1, 256, -1, -1)[FLOAT]], [590 -> (512, 256, 1, 1)[FLOAT]], [591 -> (512)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_36 for ONNX node: Conv_36\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (2, 2), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 512\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 512, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 589 for ONNX tensor: 589\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_36 [Conv] outputs: [589 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Add_37 [Add]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 586\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 589\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_37 [Add] inputs: [586 -> (-1, 512, -1, -1)[FLOAT]], [589 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Add_37 for ONNX node: Add_37\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 387 for ONNX tensor: 387\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_37 [Add] outputs: [387 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_38 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 387\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_38 [Relu] inputs: [387 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_38 for ONNX node: Relu_38\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 388 for ONNX tensor: 388\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_38 [Relu] outputs: [388 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_39 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 388\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 593\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 594\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_39 [Conv] inputs: [388 -> (-1, 512, -1, -1)[FLOAT]], [593 -> (128, 512, 1, 1)[FLOAT]], [594 -> (128)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 512, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_39 for ONNX node: Conv_39\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 128\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 128, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 592 for ONNX tensor: 592\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_39 [Conv] outputs: [592 -> (-1, 128, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_40 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 592\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_40 [Relu] inputs: [592 -> (-1, 128, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_40 for ONNX node: Relu_40\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 391 for ONNX tensor: 391\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_40 [Relu] outputs: [391 -> (-1, 128, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_41 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 391\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 596\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 597\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_41 [Conv] inputs: [391 -> (-1, 128, -1, -1)[FLOAT]], [596 -> (128, 128, 3, 3)[FLOAT]], [597 -> (128)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 128, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_41 for ONNX node: Conv_41\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 128\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 128, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 595 for ONNX tensor: 595\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_41 [Conv] outputs: [595 -> (-1, 128, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_42 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 595\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_42 [Relu] inputs: [595 -> (-1, 128, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_42 for ONNX node: Relu_42\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 394 for ONNX tensor: 394\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_42 [Relu] outputs: [394 -> (-1, 128, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_43 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 394\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 599\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 600\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_43 [Conv] inputs: [394 -> (-1, 128, -1, -1)[FLOAT]], [599 -> (512, 128, 1, 1)[FLOAT]], [600 -> (512)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 128, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_43 for ONNX node: Conv_43\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 512\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 512, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 598 for ONNX tensor: 598\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_43 [Conv] outputs: [598 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Add_44 [Add]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 598\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 388\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_44 [Add] inputs: [598 -> (-1, 512, -1, -1)[FLOAT]], [388 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Add_44 for ONNX node: Add_44\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 397 for ONNX tensor: 397\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_44 [Add] outputs: [397 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_45 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 397\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_45 [Relu] inputs: [397 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_45 for ONNX node: Relu_45\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 398 for ONNX tensor: 398\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_45 [Relu] outputs: [398 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_46 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 398\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 602\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 603\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_46 [Conv] inputs: [398 -> (-1, 512, -1, -1)[FLOAT]], [602 -> (128, 512, 1, 1)[FLOAT]], [603 -> (128)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 512, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_46 for ONNX node: Conv_46\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 128\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 128, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 601 for ONNX tensor: 601\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_46 [Conv] outputs: [601 -> (-1, 128, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_47 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 601\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_47 [Relu] inputs: [601 -> (-1, 128, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_47 for ONNX node: Relu_47\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 401 for ONNX tensor: 401\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_47 [Relu] outputs: [401 -> (-1, 128, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_48 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 401\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 605\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 606\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_48 [Conv] inputs: [401 -> (-1, 128, -1, -1)[FLOAT]], [605 -> (128, 128, 3, 3)[FLOAT]], [606 -> (128)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 128, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_48 for ONNX node: Conv_48\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 128\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 128, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 604 for ONNX tensor: 604\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_48 [Conv] outputs: [604 -> (-1, 128, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_49 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 604\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_49 [Relu] inputs: [604 -> (-1, 128, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_49 for ONNX node: Relu_49\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 404 for ONNX tensor: 404\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_49 [Relu] outputs: [404 -> (-1, 128, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_50 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 404\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 608\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 609\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_50 [Conv] inputs: [404 -> (-1, 128, -1, -1)[FLOAT]], [608 -> (512, 128, 1, 1)[FLOAT]], [609 -> (512)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 128, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_50 for ONNX node: Conv_50\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 512\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 512, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 607 for ONNX tensor: 607\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_50 [Conv] outputs: [607 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Add_51 [Add]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 607\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 398\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_51 [Add] inputs: [607 -> (-1, 512, -1, -1)[FLOAT]], [398 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Add_51 for ONNX node: Add_51\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 407 for ONNX tensor: 407\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_51 [Add] outputs: [407 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_52 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 407\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_52 [Relu] inputs: [407 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_52 for ONNX node: Relu_52\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 408 for ONNX tensor: 408\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_52 [Relu] outputs: [408 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_53 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 408\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 611\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 612\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_53 [Conv] inputs: [408 -> (-1, 512, -1, -1)[FLOAT]], [611 -> (128, 512, 1, 1)[FLOAT]], [612 -> (128)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 512, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_53 for ONNX node: Conv_53\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 128\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 128, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 610 for ONNX tensor: 610\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_53 [Conv] outputs: [610 -> (-1, 128, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_54 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 610\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_54 [Relu] inputs: [610 -> (-1, 128, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_54 for ONNX node: Relu_54\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 411 for ONNX tensor: 411\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_54 [Relu] outputs: [411 -> (-1, 128, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_55 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 411\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 614\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 615\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_55 [Conv] inputs: [411 -> (-1, 128, -1, -1)[FLOAT]], [614 -> (128, 128, 3, 3)[FLOAT]], [615 -> (128)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 128, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_55 for ONNX node: Conv_55\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 128\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 128, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 613 for ONNX tensor: 613\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_55 [Conv] outputs: [613 -> (-1, 128, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_56 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 613\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_56 [Relu] inputs: [613 -> (-1, 128, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_56 for ONNX node: Relu_56\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 414 for ONNX tensor: 414\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_56 [Relu] outputs: [414 -> (-1, 128, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_57 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 414\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 617\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 618\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_57 [Conv] inputs: [414 -> (-1, 128, -1, -1)[FLOAT]], [617 -> (512, 128, 1, 1)[FLOAT]], [618 -> (512)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 128, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_57 for ONNX node: Conv_57\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 512\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 512, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 616 for ONNX tensor: 616\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_57 [Conv] outputs: [616 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Add_58 [Add]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 616\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 408\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_58 [Add] inputs: [616 -> (-1, 512, -1, -1)[FLOAT]], [408 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Add_58 for ONNX node: Add_58\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 417 for ONNX tensor: 417\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_58 [Add] outputs: [417 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_59 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 417\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_59 [Relu] inputs: [417 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_59 for ONNX node: Relu_59\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 418 for ONNX tensor: 418\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_59 [Relu] outputs: [418 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_60 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 418\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 620\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 621\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_60 [Conv] inputs: [418 -> (-1, 512, -1, -1)[FLOAT]], [620 -> (256, 512, 1, 1)[FLOAT]], [621 -> (256)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 512, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_60 for ONNX node: Conv_60\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 619 for ONNX tensor: 619\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_60 [Conv] outputs: [619 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_61 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 619\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_61 [Relu] inputs: [619 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_61 for ONNX node: Relu_61\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 421 for ONNX tensor: 421\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_61 [Relu] outputs: [421 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_62 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 421\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 623\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 624\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_62 [Conv] inputs: [421 -> (-1, 256, -1, -1)[FLOAT]], [623 -> (256, 256, 3, 3)[FLOAT]], [624 -> (256)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_62 for ONNX node: Conv_62\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 256\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 622 for ONNX tensor: 622\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_62 [Conv] outputs: [622 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_63 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 622\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_63 [Relu] inputs: [622 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_63 for ONNX node: Relu_63\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 424 for ONNX tensor: 424\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_63 [Relu] outputs: [424 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_64 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 424\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 626\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 627\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_64 [Conv] inputs: [424 -> (-1, 256, -1, -1)[FLOAT]], [626 -> (1024, 256, 1, 1)[FLOAT]], [627 -> (1024)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_64 for ONNX node: Conv_64\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 1024\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 1024, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 625 for ONNX tensor: 625\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_64 [Conv] outputs: [625 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_65 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 418\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 629\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 630\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_65 [Conv] inputs: [418 -> (-1, 512, -1, -1)[FLOAT]], [629 -> (1024, 512, 1, 1)[FLOAT]], [630 -> (1024)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 512, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_65 for ONNX node: Conv_65\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 1024\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 1024, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 628 for ONNX tensor: 628\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_65 [Conv] outputs: [628 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Add_66 [Add]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 625\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 628\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_66 [Add] inputs: [625 -> (-1, 1024, -1, -1)[FLOAT]], [628 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Add_66 for ONNX node: Add_66\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 429 for ONNX tensor: 429\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_66 [Add] outputs: [429 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_67 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 429\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_67 [Relu] inputs: [429 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_67 for ONNX node: Relu_67\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 430 for ONNX tensor: 430\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_67 [Relu] outputs: [430 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_68 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 430\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 632\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 633\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_68 [Conv] inputs: [430 -> (-1, 1024, -1, -1)[FLOAT]], [632 -> (256, 1024, 1, 1)[FLOAT]], [633 -> (256)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 1024, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_68 for ONNX node: Conv_68\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 631 for ONNX tensor: 631\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_68 [Conv] outputs: [631 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_69 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 631\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_69 [Relu] inputs: [631 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_69 for ONNX node: Relu_69\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 433 for ONNX tensor: 433\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_69 [Relu] outputs: [433 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_70 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 433\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 635\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 636\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_70 [Conv] inputs: [433 -> (-1, 256, -1, -1)[FLOAT]], [635 -> (256, 256, 3, 3)[FLOAT]], [636 -> (256)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_70 for ONNX node: Conv_70\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (3, 3), strides: (1, 1), prepadding: (2, 2), postpadding: (2, 2), dilations: (2, 2), numOutputs: 256\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 634 for ONNX tensor: 634\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_70 [Conv] outputs: [634 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_71 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 634\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_71 [Relu] inputs: [634 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_71 for ONNX node: Relu_71\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 436 for ONNX tensor: 436\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_71 [Relu] outputs: [436 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_72 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 436\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 638\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 639\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_72 [Conv] inputs: [436 -> (-1, 256, -1, -1)[FLOAT]], [638 -> (1024, 256, 1, 1)[FLOAT]], [639 -> (1024)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_72 for ONNX node: Conv_72\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 1024\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 1024, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 637 for ONNX tensor: 637\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_72 [Conv] outputs: [637 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Add_73 [Add]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 637\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 430\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_73 [Add] inputs: [637 -> (-1, 1024, -1, -1)[FLOAT]], [430 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Add_73 for ONNX node: Add_73\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 439 for ONNX tensor: 439\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_73 [Add] outputs: [439 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_74 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 439\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_74 [Relu] inputs: [439 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_74 for ONNX node: Relu_74\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 440 for ONNX tensor: 440\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_74 [Relu] outputs: [440 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_75 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 440\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 641\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 642\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_75 [Conv] inputs: [440 -> (-1, 1024, -1, -1)[FLOAT]], [641 -> (256, 1024, 1, 1)[FLOAT]], [642 -> (256)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 1024, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_75 for ONNX node: Conv_75\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 640 for ONNX tensor: 640\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_75 [Conv] outputs: [640 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_76 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 640\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_76 [Relu] inputs: [640 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_76 for ONNX node: Relu_76\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 443 for ONNX tensor: 443\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_76 [Relu] outputs: [443 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_77 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 443\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 644\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 645\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_77 [Conv] inputs: [443 -> (-1, 256, -1, -1)[FLOAT]], [644 -> (256, 256, 3, 3)[FLOAT]], [645 -> (256)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_77 for ONNX node: Conv_77\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (3, 3), strides: (1, 1), prepadding: (2, 2), postpadding: (2, 2), dilations: (2, 2), numOutputs: 256\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 643 for ONNX tensor: 643\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_77 [Conv] outputs: [643 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_78 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 643\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_78 [Relu] inputs: [643 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_78 for ONNX node: Relu_78\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 446 for ONNX tensor: 446\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_78 [Relu] outputs: [446 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_79 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 446\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 647\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 648\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_79 [Conv] inputs: [446 -> (-1, 256, -1, -1)[FLOAT]], [647 -> (1024, 256, 1, 1)[FLOAT]], [648 -> (1024)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_79 for ONNX node: Conv_79\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 1024\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 1024, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 646 for ONNX tensor: 646\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_79 [Conv] outputs: [646 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Add_80 [Add]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 646\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 440\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_80 [Add] inputs: [646 -> (-1, 1024, -1, -1)[FLOAT]], [440 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Add_80 for ONNX node: Add_80\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 449 for ONNX tensor: 449\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_80 [Add] outputs: [449 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_81 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 449\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_81 [Relu] inputs: [449 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_81 for ONNX node: Relu_81\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 450 for ONNX tensor: 450\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_81 [Relu] outputs: [450 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_82 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 450\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 650\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 651\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_82 [Conv] inputs: [450 -> (-1, 1024, -1, -1)[FLOAT]], [650 -> (256, 1024, 1, 1)[FLOAT]], [651 -> (256)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 1024, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_82 for ONNX node: Conv_82\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 649 for ONNX tensor: 649\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_82 [Conv] outputs: [649 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_83 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 649\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_83 [Relu] inputs: [649 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_83 for ONNX node: Relu_83\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 453 for ONNX tensor: 453\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_83 [Relu] outputs: [453 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_84 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 453\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 653\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 654\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_84 [Conv] inputs: [453 -> (-1, 256, -1, -1)[FLOAT]], [653 -> (256, 256, 3, 3)[FLOAT]], [654 -> (256)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_84 for ONNX node: Conv_84\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (3, 3), strides: (1, 1), prepadding: (2, 2), postpadding: (2, 2), dilations: (2, 2), numOutputs: 256\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 652 for ONNX tensor: 652\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_84 [Conv] outputs: [652 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_85 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 652\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_85 [Relu] inputs: [652 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_85 for ONNX node: Relu_85\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 456 for ONNX tensor: 456\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_85 [Relu] outputs: [456 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_86 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 456\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 656\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 657\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_86 [Conv] inputs: [456 -> (-1, 256, -1, -1)[FLOAT]], [656 -> (1024, 256, 1, 1)[FLOAT]], [657 -> (1024)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_86 for ONNX node: Conv_86\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 1024\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 1024, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 655 for ONNX tensor: 655\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_86 [Conv] outputs: [655 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Add_87 [Add]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 655\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 450\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_87 [Add] inputs: [655 -> (-1, 1024, -1, -1)[FLOAT]], [450 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Add_87 for ONNX node: Add_87\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 459 for ONNX tensor: 459\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_87 [Add] outputs: [459 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_88 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 459\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_88 [Relu] inputs: [459 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_88 for ONNX node: Relu_88\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 460 for ONNX tensor: 460\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_88 [Relu] outputs: [460 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_89 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 460\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 659\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 660\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_89 [Conv] inputs: [460 -> (-1, 1024, -1, -1)[FLOAT]], [659 -> (256, 1024, 1, 1)[FLOAT]], [660 -> (256)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 1024, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_89 for ONNX node: Conv_89\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 658 for ONNX tensor: 658\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_89 [Conv] outputs: [658 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_90 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 658\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_90 [Relu] inputs: [658 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_90 for ONNX node: Relu_90\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 463 for ONNX tensor: 463\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_90 [Relu] outputs: [463 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_91 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 463\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 662\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 663\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_91 [Conv] inputs: [463 -> (-1, 256, -1, -1)[FLOAT]], [662 -> (256, 256, 3, 3)[FLOAT]], [663 -> (256)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_91 for ONNX node: Conv_91\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (3, 3), strides: (1, 1), prepadding: (2, 2), postpadding: (2, 2), dilations: (2, 2), numOutputs: 256\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 661 for ONNX tensor: 661\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_91 [Conv] outputs: [661 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_92 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 661\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_92 [Relu] inputs: [661 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_92 for ONNX node: Relu_92\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 466 for ONNX tensor: 466\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_92 [Relu] outputs: [466 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_93 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 466\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 665\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 666\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_93 [Conv] inputs: [466 -> (-1, 256, -1, -1)[FLOAT]], [665 -> (1024, 256, 1, 1)[FLOAT]], [666 -> (1024)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_93 for ONNX node: Conv_93\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 1024\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 1024, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 664 for ONNX tensor: 664\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_93 [Conv] outputs: [664 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Add_94 [Add]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 664\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 460\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_94 [Add] inputs: [664 -> (-1, 1024, -1, -1)[FLOAT]], [460 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Add_94 for ONNX node: Add_94\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 469 for ONNX tensor: 469\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_94 [Add] outputs: [469 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_95 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 469\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_95 [Relu] inputs: [469 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_95 for ONNX node: Relu_95\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 470 for ONNX tensor: 470\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_95 [Relu] outputs: [470 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_96 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 470\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 668\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 669\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_96 [Conv] inputs: [470 -> (-1, 1024, -1, -1)[FLOAT]], [668 -> (256, 1024, 1, 1)[FLOAT]], [669 -> (256)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 1024, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_96 for ONNX node: Conv_96\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 256\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 667 for ONNX tensor: 667\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_96 [Conv] outputs: [667 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_97 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 667\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_97 [Relu] inputs: [667 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_97 for ONNX node: Relu_97\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 473 for ONNX tensor: 473\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_97 [Relu] outputs: [473 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_98 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 473\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 671\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 672\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_98 [Conv] inputs: [473 -> (-1, 256, -1, -1)[FLOAT]], [671 -> (256, 256, 3, 3)[FLOAT]], [672 -> (256)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_98 for ONNX node: Conv_98\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (3, 3), strides: (1, 1), prepadding: (2, 2), postpadding: (2, 2), dilations: (2, 2), numOutputs: 256\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 670 for ONNX tensor: 670\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_98 [Conv] outputs: [670 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_99 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 670\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_99 [Relu] inputs: [670 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_99 for ONNX node: Relu_99\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 476 for ONNX tensor: 476\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_99 [Relu] outputs: [476 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_100 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 476\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 674\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 675\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_100 [Conv] inputs: [476 -> (-1, 256, -1, -1)[FLOAT]], [674 -> (1024, 256, 1, 1)[FLOAT]], [675 -> (1024)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_100 for ONNX node: Conv_100\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 1024\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 1024, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 673 for ONNX tensor: 673\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_100 [Conv] outputs: [673 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Add_101 [Add]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 673\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 470\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_101 [Add] inputs: [673 -> (-1, 1024, -1, -1)[FLOAT]], [470 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Add_101 for ONNX node: Add_101\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 479 for ONNX tensor: 479\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_101 [Add] outputs: [479 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_102 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 479\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_102 [Relu] inputs: [479 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_102 for ONNX node: Relu_102\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 480 for ONNX tensor: 480\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_102 [Relu] outputs: [480 -> (-1, 1024, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_103 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 480\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 677\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 678\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_103 [Conv] inputs: [480 -> (-1, 1024, -1, -1)[FLOAT]], [677 -> (512, 1024, 1, 1)[FLOAT]], [678 -> (512)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 1024, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_103 for ONNX node: Conv_103\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 512\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 512, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 676 for ONNX tensor: 676\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_103 [Conv] outputs: [676 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_104 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 676\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_104 [Relu] inputs: [676 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_104 for ONNX node: Relu_104\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 483 for ONNX tensor: 483\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_104 [Relu] outputs: [483 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_105 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 483\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 680\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 681\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_105 [Conv] inputs: [483 -> (-1, 512, -1, -1)[FLOAT]], [680 -> (512, 512, 3, 3)[FLOAT]], [681 -> (512)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 512, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_105 for ONNX node: Conv_105\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (3, 3), strides: (1, 1), prepadding: (2, 2), postpadding: (2, 2), dilations: (2, 2), numOutputs: 512\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 512, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 679 for ONNX tensor: 679\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_105 [Conv] outputs: [679 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_106 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 679\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_106 [Relu] inputs: [679 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_106 for ONNX node: Relu_106\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 486 for ONNX tensor: 486\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_106 [Relu] outputs: [486 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_107 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 486\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 683\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 684\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_107 [Conv] inputs: [486 -> (-1, 512, -1, -1)[FLOAT]], [683 -> (2048, 512, 1, 1)[FLOAT]], [684 -> (2048)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 512, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_107 for ONNX node: Conv_107\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 2048\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 2048, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 682 for ONNX tensor: 682\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_107 [Conv] outputs: [682 -> (-1, 2048, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_108 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 480\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 686\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 687\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_108 [Conv] inputs: [480 -> (-1, 1024, -1, -1)[FLOAT]], [686 -> (2048, 1024, 1, 1)[FLOAT]], [687 -> (2048)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 1024, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_108 for ONNX node: Conv_108\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 2048\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 2048, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 685 for ONNX tensor: 685\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_108 [Conv] outputs: [685 -> (-1, 2048, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Add_109 [Add]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 682\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 685\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_109 [Add] inputs: [682 -> (-1, 2048, -1, -1)[FLOAT]], [685 -> (-1, 2048, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Add_109 for ONNX node: Add_109\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 491 for ONNX tensor: 491\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_109 [Add] outputs: [491 -> (-1, 2048, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_110 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 491\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_110 [Relu] inputs: [491 -> (-1, 2048, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_110 for ONNX node: Relu_110\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 492 for ONNX tensor: 492\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_110 [Relu] outputs: [492 -> (-1, 2048, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_111 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 492\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 689\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 690\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_111 [Conv] inputs: [492 -> (-1, 2048, -1, -1)[FLOAT]], [689 -> (512, 2048, 1, 1)[FLOAT]], [690 -> (512)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 2048, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_111 for ONNX node: Conv_111\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 512\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 512, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 688 for ONNX tensor: 688\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_111 [Conv] outputs: [688 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_112 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 688\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_112 [Relu] inputs: [688 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_112 for ONNX node: Relu_112\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 495 for ONNX tensor: 495\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_112 [Relu] outputs: [495 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_113 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 495\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 692\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 693\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_113 [Conv] inputs: [495 -> (-1, 512, -1, -1)[FLOAT]], [692 -> (512, 512, 3, 3)[FLOAT]], [693 -> (512)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 512, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_113 for ONNX node: Conv_113\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (3, 3), strides: (1, 1), prepadding: (4, 4), postpadding: (4, 4), dilations: (4, 4), numOutputs: 512\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 512, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 691 for ONNX tensor: 691\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_113 [Conv] outputs: [691 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_114 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 691\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_114 [Relu] inputs: [691 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_114 for ONNX node: Relu_114\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 498 for ONNX tensor: 498\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_114 [Relu] outputs: [498 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_115 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 498\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 695\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 696\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_115 [Conv] inputs: [498 -> (-1, 512, -1, -1)[FLOAT]], [695 -> (2048, 512, 1, 1)[FLOAT]], [696 -> (2048)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 512, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_115 for ONNX node: Conv_115\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 2048\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 2048, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 694 for ONNX tensor: 694\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_115 [Conv] outputs: [694 -> (-1, 2048, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Add_116 [Add]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 694\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 492\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_116 [Add] inputs: [694 -> (-1, 2048, -1, -1)[FLOAT]], [492 -> (-1, 2048, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Add_116 for ONNX node: Add_116\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 501 for ONNX tensor: 501\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_116 [Add] outputs: [501 -> (-1, 2048, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_117 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 501\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_117 [Relu] inputs: [501 -> (-1, 2048, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_117 for ONNX node: Relu_117\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 502 for ONNX tensor: 502\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_117 [Relu] outputs: [502 -> (-1, 2048, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_118 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 502\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 698\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 699\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_118 [Conv] inputs: [502 -> (-1, 2048, -1, -1)[FLOAT]], [698 -> (512, 2048, 1, 1)[FLOAT]], [699 -> (512)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 2048, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_118 for ONNX node: Conv_118\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 512\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 512, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 697 for ONNX tensor: 697\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_118 [Conv] outputs: [697 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_119 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 697\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_119 [Relu] inputs: [697 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_119 for ONNX node: Relu_119\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 505 for ONNX tensor: 505\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_119 [Relu] outputs: [505 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_120 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 505\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 701\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 702\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_120 [Conv] inputs: [505 -> (-1, 512, -1, -1)[FLOAT]], [701 -> (512, 512, 3, 3)[FLOAT]], [702 -> (512)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 512, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_120 for ONNX node: Conv_120\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (3, 3), strides: (1, 1), prepadding: (4, 4), postpadding: (4, 4), dilations: (4, 4), numOutputs: 512\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 512, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 700 for ONNX tensor: 700\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_120 [Conv] outputs: [700 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_121 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 700\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_121 [Relu] inputs: [700 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_121 for ONNX node: Relu_121\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 508 for ONNX tensor: 508\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_121 [Relu] outputs: [508 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_122 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 508\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 704\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 705\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_122 [Conv] inputs: [508 -> (-1, 512, -1, -1)[FLOAT]], [704 -> (2048, 512, 1, 1)[FLOAT]], [705 -> (2048)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 512, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_122 for ONNX node: Conv_122\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 2048\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 2048, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 703 for ONNX tensor: 703\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_122 [Conv] outputs: [703 -> (-1, 2048, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Add_123 [Add]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 703\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 502\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_123 [Add] inputs: [703 -> (-1, 2048, -1, -1)[FLOAT]], [502 -> (-1, 2048, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Add_123 for ONNX node: Add_123\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 511 for ONNX tensor: 511\n",
      "[01/11/2022-07:43:36] [TRT] [V] Add_123 [Add] outputs: [511 -> (-1, 2048, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_124 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 511\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_124 [Relu] inputs: [511 -> (-1, 2048, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_124 for ONNX node: Relu_124\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 512 for ONNX tensor: 512\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_124 [Relu] outputs: [512 -> (-1, 2048, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_125 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 512\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 707\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 708\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_125 [Conv] inputs: [512 -> (-1, 2048, -1, -1)[FLOAT]], [707 -> (512, 2048, 3, 3)[FLOAT]], [708 -> (512)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 2048, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_125 for ONNX node: Conv_125\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 512\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 512, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 706 for ONNX tensor: 706\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_125 [Conv] outputs: [706 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_126 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 706\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_126 [Relu] inputs: [706 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_126 for ONNX node: Relu_126\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 515 for ONNX tensor: 515\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_126 [Relu] outputs: [515 -> (-1, 512, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_127 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 515\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: classifier.4.weight\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: classifier.4.bias\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_127 [Conv] inputs: [515 -> (-1, 512, -1, -1)[FLOAT]], [classifier.4.weight -> (21, 512, 1, 1)[FLOAT]], [classifier.4.bias -> (21)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 512, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_127 for ONNX node: Conv_127\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 21\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 21, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 516 for ONNX tensor: 516\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_127 [Conv] outputs: [516 -> (-1, 21, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Unsqueeze_128 [Unsqueeze]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 337\n",
      "[01/11/2022-07:43:36] [TRT] [V] Unsqueeze_128 [Unsqueeze] inputs: [337 -> ()[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Original shape: (), unsqueezing to: (1,)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Unsqueeze_128 for ONNX node: Unsqueeze_128\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 517 for ONNX tensor: 517\n",
      "[01/11/2022-07:43:36] [TRT] [V] Unsqueeze_128 [Unsqueeze] outputs: [517 -> (1)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Unsqueeze_129 [Unsqueeze]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 340\n",
      "[01/11/2022-07:43:36] [TRT] [V] Unsqueeze_129 [Unsqueeze] inputs: [340 -> ()[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Original shape: (), unsqueezing to: (1,)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Unsqueeze_129 for ONNX node: Unsqueeze_129\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 518 for ONNX tensor: 518\n",
      "[01/11/2022-07:43:36] [TRT] [V] Unsqueeze_129 [Unsqueeze] outputs: [518 -> (1)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Concat_130 [Concat]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 517\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 518\n",
      "[01/11/2022-07:43:36] [TRT] [V] Concat_130 [Concat] inputs: [517 -> (1)[INT32]], [518 -> (1)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Concat_130 for ONNX node: Concat_130\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 519 for ONNX tensor: 519\n",
      "[01/11/2022-07:43:36] [TRT] [V] Concat_130 [Concat] outputs: [519 -> (2)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Shape_131 [Shape]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 516\n",
      "[01/11/2022-07:43:36] [TRT] [V] Shape_131 [Shape] inputs: [516 -> (-1, 21, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Shape_131 for ONNX node: Shape_131\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 520 for ONNX tensor: 520\n",
      "[01/11/2022-07:43:36] [TRT] [V] Shape_131 [Shape] outputs: [520 -> (4)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Constant_132 [Constant]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Constant_132 [Constant] inputs: \n",
      "[01/11/2022-07:43:36] [TRT] [V] Constant_132 [Constant] outputs: [521 -> (1)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Constant_133 [Constant]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Constant_133 [Constant] inputs: \n",
      "[01/11/2022-07:43:36] [TRT] [V] Constant_133 [Constant] outputs: [522 -> (1)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Constant_134 [Constant]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Constant_134 [Constant] inputs: \n",
      "[01/11/2022-07:43:36] [TRT] [V] Constant_134 [Constant] outputs: [523 -> (1)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Slice_135 [Slice]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 520\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 522\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 523\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 521\n",
      "[01/11/2022-07:43:36] [TRT] [V] Slice_135 [Slice] inputs: [520 -> (4)[INT32]], [522 -> (1)[INT32]], [523 -> (1)[INT32]], [521 -> (1)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Slice_135 for ONNX node: Slice_135\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 524 for ONNX tensor: 524\n",
      "[01/11/2022-07:43:36] [TRT] [V] Slice_135 [Slice] outputs: [524 -> (2)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Cast_136 [Cast]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 519\n",
      "[01/11/2022-07:43:36] [TRT] [V] Cast_136 [Cast] inputs: [519 -> (2)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Casting to type: int32\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Cast_136 for ONNX node: Cast_136\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 525 for ONNX tensor: 525\n",
      "[01/11/2022-07:43:36] [TRT] [V] Cast_136 [Cast] outputs: [525 -> (2)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Concat_137 [Concat]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 524\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 525\n",
      "[01/11/2022-07:43:36] [TRT] [V] Concat_137 [Concat] inputs: [524 -> (2)[INT32]], [525 -> (2)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Concat_137 for ONNX node: Concat_137\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 526 for ONNX tensor: 526\n",
      "[01/11/2022-07:43:36] [TRT] [V] Concat_137 [Concat] outputs: [526 -> (4)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Constant_138 [Constant]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Constant_138 [Constant] inputs: \n",
      "[01/11/2022-07:43:36] [TRT] [V] Constant_138 [Constant] outputs: [527 -> ()[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Constant_139 [Constant]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Constant_139 [Constant] inputs: \n",
      "[01/11/2022-07:43:36] [TRT] [V] Constant_139 [Constant] outputs: [528 -> ()[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Resize_140 [Resize]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 516\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 527\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 528\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 526\n",
      "[01/11/2022-07:43:36] [TRT] [V] Resize_140 [Resize] inputs: [516 -> (-1, 21, -1, -1)[FLOAT]], [527 -> ()[FLOAT]], [528 -> ()[FLOAT]], [526 -> (4)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Resize_140 for ONNX node: Resize_140\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: dense_out_0 for ONNX tensor: dense_out\n",
      "[01/11/2022-07:43:36] [TRT] [V] Resize_140 [Resize] outputs: [dense_out -> (-1, 21, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_141 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 480\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 710\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 711\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_141 [Conv] inputs: [480 -> (-1, 1024, -1, -1)[FLOAT]], [710 -> (256, 1024, 3, 3)[FLOAT]], [711 -> (256)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 1024, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_141 for ONNX node: Conv_141\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (3, 3), strides: (1, 1), prepadding: (1, 1), postpadding: (1, 1), dilations: (1, 1), numOutputs: 256\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 709 for ONNX tensor: 709\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_141 [Conv] outputs: [709 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Relu_142 [Relu]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 709\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_142 [Relu] inputs: [709 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Relu_142 for ONNX node: Relu_142\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 532 for ONNX tensor: 532\n",
      "[01/11/2022-07:43:36] [TRT] [V] Relu_142 [Relu] outputs: [532 -> (-1, 256, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Conv_143 [Conv]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 532\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: aux_classifier.4.weight\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: aux_classifier.4.bias\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_143 [Conv] inputs: [532 -> (-1, 256, -1, -1)[FLOAT]], [aux_classifier.4.weight -> (21, 256, 1, 1)[FLOAT]], [aux_classifier.4.bias -> (21)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution input dimensions: (-1, 256, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Conv_143 for ONNX node: Conv_143\n",
      "[01/11/2022-07:43:36] [TRT] [V] Using kernel: (1, 1), strides: (1, 1), prepadding: (0, 0), postpadding: (0, 0), dilations: (1, 1), numOutputs: 21\n",
      "[01/11/2022-07:43:36] [TRT] [V] Convolution output dimensions: (-1, 21, -1, -1)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 533 for ONNX tensor: 533\n",
      "[01/11/2022-07:43:36] [TRT] [V] Conv_143 [Conv] outputs: [533 -> (-1, 21, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Unsqueeze_144 [Unsqueeze]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 337\n",
      "[01/11/2022-07:43:36] [TRT] [V] Unsqueeze_144 [Unsqueeze] inputs: [337 -> ()[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Original shape: (), unsqueezing to: (1,)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Unsqueeze_144 for ONNX node: Unsqueeze_144\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 534 for ONNX tensor: 534\n",
      "[01/11/2022-07:43:36] [TRT] [V] Unsqueeze_144 [Unsqueeze] outputs: [534 -> (1)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Unsqueeze_145 [Unsqueeze]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 340\n",
      "[01/11/2022-07:43:36] [TRT] [V] Unsqueeze_145 [Unsqueeze] inputs: [340 -> ()[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Original shape: (), unsqueezing to: (1,)\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Unsqueeze_145 for ONNX node: Unsqueeze_145\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 535 for ONNX tensor: 535\n",
      "[01/11/2022-07:43:36] [TRT] [V] Unsqueeze_145 [Unsqueeze] outputs: [535 -> (1)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Concat_146 [Concat]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 534\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 535\n",
      "[01/11/2022-07:43:36] [TRT] [V] Concat_146 [Concat] inputs: [534 -> (1)[INT32]], [535 -> (1)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Concat_146 for ONNX node: Concat_146\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 536 for ONNX tensor: 536\n",
      "[01/11/2022-07:43:36] [TRT] [V] Concat_146 [Concat] outputs: [536 -> (2)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Shape_147 [Shape]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 533\n",
      "[01/11/2022-07:43:36] [TRT] [V] Shape_147 [Shape] inputs: [533 -> (-1, 21, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Shape_147 for ONNX node: Shape_147\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 537 for ONNX tensor: 537\n",
      "[01/11/2022-07:43:36] [TRT] [V] Shape_147 [Shape] outputs: [537 -> (4)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Constant_148 [Constant]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Constant_148 [Constant] inputs: \n",
      "[01/11/2022-07:43:36] [TRT] [V] Constant_148 [Constant] outputs: [538 -> (1)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Constant_149 [Constant]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Constant_149 [Constant] inputs: \n",
      "[01/11/2022-07:43:36] [TRT] [V] Constant_149 [Constant] outputs: [539 -> (1)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Constant_150 [Constant]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Constant_150 [Constant] inputs: \n",
      "[01/11/2022-07:43:36] [TRT] [V] Constant_150 [Constant] outputs: [540 -> (1)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Slice_151 [Slice]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 537\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 539\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 540\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 538\n",
      "[01/11/2022-07:43:36] [TRT] [V] Slice_151 [Slice] inputs: [537 -> (4)[INT32]], [539 -> (1)[INT32]], [540 -> (1)[INT32]], [538 -> (1)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Slice_151 for ONNX node: Slice_151\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 541 for ONNX tensor: 541\n",
      "[01/11/2022-07:43:36] [TRT] [V] Slice_151 [Slice] outputs: [541 -> (2)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Cast_152 [Cast]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 536\n",
      "[01/11/2022-07:43:36] [TRT] [V] Cast_152 [Cast] inputs: [536 -> (2)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Casting to type: int32\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Cast_152 for ONNX node: Cast_152\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 542 for ONNX tensor: 542\n",
      "[01/11/2022-07:43:36] [TRT] [V] Cast_152 [Cast] outputs: [542 -> (2)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Concat_153 [Concat]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 541\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 542\n",
      "[01/11/2022-07:43:36] [TRT] [V] Concat_153 [Concat] inputs: [541 -> (2)[INT32]], [542 -> (2)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Concat_153 for ONNX node: Concat_153\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 543 for ONNX tensor: 543\n",
      "[01/11/2022-07:43:36] [TRT] [V] Concat_153 [Concat] outputs: [543 -> (4)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Constant_154 [Constant]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Constant_154 [Constant] inputs: \n",
      "[01/11/2022-07:43:36] [TRT] [V] Constant_154 [Constant] outputs: [544 -> ()[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Constant_155 [Constant]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Constant_155 [Constant] inputs: \n",
      "[01/11/2022-07:43:36] [TRT] [V] Constant_155 [Constant] outputs: [545 -> ()[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Parsing node: Resize_156 [Resize]\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 533\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 544\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 545\n",
      "[01/11/2022-07:43:36] [TRT] [V] Searching for input: 543\n",
      "[01/11/2022-07:43:36] [TRT] [V] Resize_156 [Resize] inputs: [533 -> (-1, 21, -1, -1)[FLOAT]], [544 -> ()[FLOAT]], [545 -> ()[FLOAT]], [543 -> (4)[INT32]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering layer: Resize_156 for ONNX node: Resize_156\n",
      "[01/11/2022-07:43:36] [TRT] [V] Registering tensor: 546_1 for ONNX tensor: 546\n",
      "[01/11/2022-07:43:36] [TRT] [V] Resize_156 [Resize] outputs: [546 -> (-1, 21, -1, -1)[FLOAT]], \n",
      "[01/11/2022-07:43:36] [TRT] [V] Marking dense_out_0 as output: dense_out\n",
      "[01/11/2022-07:43:36] [TRT] [V] Marking 546_1 as output: 546\n",
      "--- Starting to build engine! ---\n",
      "[01/11/2022-07:43:36] [TRT] [V] Original: 127 layers\n",
      "[01/11/2022-07:43:36] [TRT] [V] After dead-layer removal: 127 layers\n",
      "[01/11/2022-07:43:36] [TRT] [V] After Myelin optimization: 127 layers\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_7 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_10 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_12 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_16 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_18 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_20 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_23 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_25 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_27 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_30 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_32 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_34 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_38 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_40 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_42 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_45 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_47 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_49 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_52 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_54 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_56 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_59 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_61 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_63 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_67 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_69 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_71 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_74 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_76 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_78 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_81 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_83 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_85 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_88 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_90 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_92 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_95 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_97 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_99 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_102 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_104 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_142 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_106 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_110 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_112 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_114 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_117 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_119 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_121 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_124 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] Running: ActivationToPointwiseConversion\n",
      "[01/11/2022-07:43:36] [TRT] [V] Swap the layer type of Relu_126 from ACTIVATION to POINTWISE\n",
      "[01/11/2022-07:43:36] [TRT] [V] After vertical fusions: 127 layers\n",
      "[01/11/2022-07:43:36] [TRT] [V] After final dead-layer removal: 127 layers\n",
      "[01/11/2022-07:43:36] [TRT] [V] After concat removal: 127 layers\n",
      "[01/11/2022-07:43:36] [TRT] [V] After tensor merging: 127 layers\n",
      "[01/11/2022-07:43:37] [TRT] [V] Using cublasLt as a tactic source\n",
      "[01/11/2022-07:43:37] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +33, GPU +72, now: CPU 4236, GPU 6174 (MiB)\n",
      "[01/11/2022-07:43:37] [TRT] [V] Using cuDNN as a tactic source\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2082/913249186.py:60: DeprecationWarning: Use build_serialized_network instead.\n",
      "  engine = builder.build_engine(network, config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/11/2022-07:43:37] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +37, GPU +58, now: CPU 4273, GPU 6232 (MiB)\n",
      "[01/11/2022-07:43:37] [TRT] [I] Timing cache disabled. Turning it on will improve builder speed.\n",
      "[01/11/2022-07:43:37] [TRT] [V] Constructing calibration profile.\n",
      "[01/11/2022-07:43:37] [TRT] [V] Reserving memory for activation tensors. Host: 0 bytes Device: 1509949440 bytes\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing reformatting costs\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:37] [TRT] [V] *************** Autotuning format combination: Float(786432,262144,512,1) -> Float(4194304,65536,256,1) ***************\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: Conv_6 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:37] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: Conv_6 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:37] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: Conv_6 (CudnnConvolution)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/11/2022-07:43:37] [TRT] [V] Tactic: 112 Time: 4.9193\n",
      "[01/11/2022-07:43:37] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:37] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:37] [TRT] [V] *************** Autotuning format combination: Float(4194304,65536,256,1) -> Float(4194304,65536,256,1) ***************\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: PWN(Relu_7) (PointWiseV2)\n",
      "[01/11/2022-07:43:37] [TRT] [V] Tactic: 0 Time: 1.5657\n",
      "[01/11/2022-07:43:37] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:37] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:37] [TRT] [V] *************** Autotuning format combination: Float(4194304,65536,256,1) -> Float(1048576,16384,128,1) ***************\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: MaxPool_8 (TiledPooling)\n",
      "[01/11/2022-07:43:37] [TRT] [V] Tactic: 257 Time: 3.09658\n",
      "[01/11/2022-07:43:37] [TRT] [V] Tactic: 257 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:37] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: TiledPooling Tactic: 257\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:37] [TRT] [V] *************** Autotuning format combination: Float(1048576,16384,128,1) -> Float(1048576,16384,128,1) ***************\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: Conv_9 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:37] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: Conv_9 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:37] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: Conv_9 (CudnnConvolution)\n",
      "[01/11/2022-07:43:37] [TRT] [V] Tactic: 112 Time: 0.999584\n",
      "[01/11/2022-07:43:37] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:37] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:37] [TRT] [V] *************** Autotuning format combination: Float(1048576,16384,128,1) -> Float(4194304,16384,128,1) ***************\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: Conv_14 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:37] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: Conv_14 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:37] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: Conv_14 (CudnnConvolution)\n",
      "[01/11/2022-07:43:37] [TRT] [V] Tactic: 112 Time: 3.29011\n",
      "[01/11/2022-07:43:37] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:37] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:37] [TRT] [V] *************** Autotuning format combination: Float(1048576,16384,128,1) -> Float(1048576,16384,128,1) ***************\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: PWN(Relu_10) (PointWiseV2)\n",
      "[01/11/2022-07:43:37] [TRT] [V] Tactic: 0 Time: 0.395264\n",
      "[01/11/2022-07:43:37] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:37] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:37] [TRT] [V] *************** Autotuning format combination: Float(1048576,16384,128,1) -> Float(1048576,16384,128,1) ***************\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: Conv_11 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:37] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: Conv_11 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:37] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: Conv_11 (CudnnConvolution)\n",
      "[01/11/2022-07:43:37] [TRT] [V] Tactic: 112 Time: 3.4816\n",
      "[01/11/2022-07:43:37] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:37] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:37] [TRT] [V] *************** Autotuning format combination: Float(1048576,16384,128,1) -> Float(1048576,16384,128,1) ***************\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: PWN(Relu_12) (PointWiseV2)\n",
      "[01/11/2022-07:43:37] [TRT] [V] Tactic: 0 Time: 0.397312\n",
      "[01/11/2022-07:43:37] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:37] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:37] [TRT] [V] *************** Autotuning format combination: Float(1048576,16384,128,1) -> Float(4194304,16384,128,1) ***************\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: Conv_13 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:37] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: Conv_13 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:37] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: Conv_13 (CudnnConvolution)\n",
      "[01/11/2022-07:43:37] [TRT] [V] Tactic: 112 Time: 3.28909\n",
      "[01/11/2022-07:43:37] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:37] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:37] [TRT] [V] *************** Autotuning format combination: Float(4194304,16384,128,1), Float(4194304,16384,128,1) -> Float(4194304,16384,128,1) ***************\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: Add_15 (ElementWise)\n",
      "[01/11/2022-07:43:37] [TRT] [V] Tactic: 1 Time: 2.33882\n",
      "[01/11/2022-07:43:37] [TRT] [V] Tactic: 1 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:37] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: ElementWise Tactic: 1\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:37] [TRT] [V] *************** Autotuning format combination: Float(4194304,16384,128,1) -> Float(4194304,16384,128,1) ***************\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: PWN(Relu_16) (PointWiseV2)\n",
      "[01/11/2022-07:43:37] [TRT] [V] Tactic: 0 Time: 1.56467\n",
      "[01/11/2022-07:43:37] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:37] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:37] [TRT] [V] *************** Autotuning format combination: Float(4194304,16384,128,1) -> Float(1048576,16384,128,1) ***************\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: Conv_17 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:37] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: Conv_17 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:37] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: Conv_17 (CudnnConvolution)\n",
      "[01/11/2022-07:43:37] [TRT] [V] Tactic: 112 Time: 2.22413\n",
      "[01/11/2022-07:43:37] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:37] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:37] [TRT] [V] *************** Autotuning format combination: Float(1048576,16384,128,1) -> Float(1048576,16384,128,1) ***************\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: PWN(Relu_18) (PointWiseV2)\n",
      "[01/11/2022-07:43:37] [TRT] [V] Tactic: 0 Time: 0.395264\n",
      "[01/11/2022-07:43:37] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:37] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:37] [TRT] [V] *************** Autotuning format combination: Float(1048576,16384,128,1) -> Float(1048576,16384,128,1) ***************\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: Conv_19 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:37] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: Conv_19 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:37] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: Conv_19 (CudnnConvolution)\n",
      "[01/11/2022-07:43:37] [TRT] [V] Tactic: 112 Time: 3.48451\n",
      "[01/11/2022-07:43:37] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:37] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:37] [TRT] [V] *************** Autotuning format combination: Float(1048576,16384,128,1) -> Float(1048576,16384,128,1) ***************\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: PWN(Relu_20) (PointWiseV2)\n",
      "[01/11/2022-07:43:37] [TRT] [V] Tactic: 0 Time: 0.396288\n",
      "[01/11/2022-07:43:37] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:37] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:37] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:37] [TRT] [V] *************** Autotuning format combination: Float(1048576,16384,128,1) -> Float(4194304,16384,128,1) ***************\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: Conv_21 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:37] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: Conv_21 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:37] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:37] [TRT] [V] --------------- Timing Runner: Conv_21 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 3.28397\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(4194304,16384,128,1), Float(4194304,16384,128,1) -> Float(4194304,16384,128,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Add_22 (ElementWise)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 1 Time: 2.34394\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 1 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: ElementWise Tactic: 1\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(4194304,16384,128,1) -> Float(4194304,16384,128,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_23) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 1.5657\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(4194304,16384,128,1) -> Float(1048576,16384,128,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_24 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_24 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_24 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 2.23334\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(1048576,16384,128,1) -> Float(1048576,16384,128,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_25) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 0.395264\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(1048576,16384,128,1) -> Float(1048576,16384,128,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_26 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_26 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_26 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 3.48262\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(1048576,16384,128,1) -> Float(1048576,16384,128,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_27) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 0.396128\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(1048576,16384,128,1) -> Float(4194304,16384,128,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_28 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_28 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_28 (CudnnConvolution)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 3.28499\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(4194304,16384,128,1), Float(4194304,16384,128,1) -> Float(4194304,16384,128,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Add_29 (ElementWise)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 1 Time: 2.34394\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 1 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: ElementWise Tactic: 1\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(4194304,16384,128,1) -> Float(4194304,16384,128,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_30) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 1.56467\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(4194304,16384,128,1) -> Float(2097152,16384,128,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_31 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_31 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_31 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 3.10067\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(4194304,16384,128,1) -> Float(2097152,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_36 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_36 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_36 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 2.90202\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(2097152,16384,128,1) -> Float(2097152,16384,128,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_32) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 0.784384\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(2097152,16384,128,1) -> Float(524288,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_33 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_33 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_33 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 2.55078\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(524288,4096,64,1) -> Float(524288,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_34) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 0.200704\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(524288,4096,64,1) -> Float(2097152,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_35 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_35 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_35 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 2.75251\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(2097152,4096,64,1), Float(2097152,4096,64,1) -> Float(2097152,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Add_37 (ElementWise)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 1 Time: 1.17146\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 1 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: ElementWise Tactic: 1\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(2097152,4096,64,1) -> Float(2097152,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_38) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 0.78848\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(2097152,4096,64,1) -> Float(524288,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_39 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_39 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_39 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 1.29536\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(524288,4096,64,1) -> Float(524288,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_40) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 0.200704\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(524288,4096,64,1) -> Float(524288,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_41 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_41 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_41 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 2.76992\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(524288,4096,64,1) -> Float(524288,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_42) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 0.19968\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(524288,4096,64,1) -> Float(2097152,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_43 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_43 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_43 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 2.75251\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(2097152,4096,64,1), Float(2097152,4096,64,1) -> Float(2097152,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Add_44 (ElementWise)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 1 Time: 1.17453\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 1 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: ElementWise Tactic: 1\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(2097152,4096,64,1) -> Float(2097152,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_45) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 0.786432\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(2097152,4096,64,1) -> Float(524288,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_46 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_46 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_46 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 1.29843\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(524288,4096,64,1) -> Float(524288,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_47) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 0.201728\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(524288,4096,64,1) -> Float(524288,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_48 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_48 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_48 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 2.49242\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(524288,4096,64,1) -> Float(524288,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_49) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 0.200704\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(524288,4096,64,1) -> Float(2097152,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_50 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_50 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_50 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 2.74944\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(2097152,4096,64,1), Float(2097152,4096,64,1) -> Float(2097152,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Add_51 (ElementWise)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 1 Time: 1.17248\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 1 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: ElementWise Tactic: 1\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(2097152,4096,64,1) -> Float(2097152,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_52) (PointWiseV2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 0.784512\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(2097152,4096,64,1) -> Float(524288,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_53 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_53 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_53 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 1.29638\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(524288,4096,64,1) -> Float(524288,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_54) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 0.201728\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(524288,4096,64,1) -> Float(524288,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_55 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_55 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_55 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 2.49773\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(524288,4096,64,1) -> Float(524288,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_56) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 0.200704\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(524288,4096,64,1) -> Float(2097152,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_57 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_57 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_57 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 2.74739\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(2097152,4096,64,1), Float(2097152,4096,64,1) -> Float(2097152,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Add_58 (ElementWise)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 1 Time: 1.17248\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 1 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: ElementWise Tactic: 1\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(2097152,4096,64,1) -> Float(2097152,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_59) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 0.785408\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(2097152,4096,64,1) -> Float(1048576,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_60 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_60 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_60 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 2.19136\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(2097152,4096,64,1) -> Float(4194304,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_65 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_65 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_65 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 8.45414\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(1048576,4096,64,1) -> Float(1048576,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_61) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 0.39424\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(1048576,4096,64,1) -> Float(1048576,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_62 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_62 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_62 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 7.82112\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(1048576,4096,64,1) -> Float(1048576,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_63) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 0.395264\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(1048576,4096,64,1) -> Float(4194304,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_64 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_64 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_64 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 6.0969\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(4194304,4096,64,1), Float(4194304,4096,64,1) -> Float(4194304,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Add_66 (ElementWise)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 1 Time: 2.34086\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 1 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: ElementWise Tactic: 1\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(4194304,4096,64,1) -> Float(4194304,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_67) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 1.56672\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(4194304,4096,64,1) -> Float(1048576,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_68 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_68 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_68 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 3.72736\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(1048576,4096,64,1) -> Float(1048576,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_69) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 0.396288\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(1048576,4096,64,1) -> Float(1048576,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_70 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_70 (CudnnConvolution)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 7.49363\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(1048576,4096,64,1) -> Float(1048576,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_71) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 0.395264\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(1048576,4096,64,1) -> Float(4194304,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_72 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_72 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_72 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 6.2167\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(4194304,4096,64,1), Float(4194304,4096,64,1) -> Float(4194304,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Add_73 (ElementWise)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 1 Time: 2.34598\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 1 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: ElementWise Tactic: 1\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(4194304,4096,64,1) -> Float(4194304,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_74) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 1.5616\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(4194304,4096,64,1) -> Float(1048576,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_75 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_75 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_75 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 3.74886\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(1048576,4096,64,1) -> Float(1048576,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_76) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 0.395264\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(1048576,4096,64,1) -> Float(1048576,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_77 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_77 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 7.5735\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(1048576,4096,64,1) -> Float(1048576,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_78) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 0.39536\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(1048576,4096,64,1) -> Float(4194304,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_79 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_79 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_79 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 5.71597\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(4194304,4096,64,1), Float(4194304,4096,64,1) -> Float(4194304,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Add_80 (ElementWise)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 1 Time: 2.34394\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 1 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: ElementWise Tactic: 1\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(4194304,4096,64,1) -> Float(4194304,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_81) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 1.56365\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(4194304,4096,64,1) -> Float(1048576,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_82 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_82 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_82 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 3.8144\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(1048576,4096,64,1) -> Float(1048576,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_83) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 0.395264\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(1048576,4096,64,1) -> Float(1048576,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_84 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_84 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 7.51206\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(1048576,4096,64,1) -> Float(1048576,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_85) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 0.395264\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(1048576,4096,64,1) -> Float(4194304,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_86 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_86 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_86 (CudnnConvolution)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 5.63507\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(4194304,4096,64,1), Float(4194304,4096,64,1) -> Float(4194304,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Add_87 (ElementWise)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 1 Time: 2.33779\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 1 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: ElementWise Tactic: 1\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(4194304,4096,64,1) -> Float(4194304,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_88) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 1.56672\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(4194304,4096,64,1) -> Float(1048576,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_89 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_89 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_89 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 3.75091\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(1048576,4096,64,1) -> Float(1048576,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_90) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 0.396288\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(1048576,4096,64,1) -> Float(1048576,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_91 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_91 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 7.45677\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(1048576,4096,64,1) -> Float(1048576,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_92) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 0.396288\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(1048576,4096,64,1) -> Float(4194304,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_93 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_93 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_93 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 5.85114\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(4194304,4096,64,1), Float(4194304,4096,64,1) -> Float(4194304,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Add_94 (ElementWise)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 1 Time: 2.33984\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 1 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: ElementWise Tactic: 1\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(4194304,4096,64,1) -> Float(4194304,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_95) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 1.56384\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(4194304,4096,64,1) -> Float(1048576,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_96 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_96 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_96 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 3.72326\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(1048576,4096,64,1) -> Float(1048576,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_97) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 0.39424\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(1048576,4096,64,1) -> Float(1048576,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_98 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_98 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 7.54893\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(1048576,4096,64,1) -> Float(1048576,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_99) (PointWiseV2)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 0.39424\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(1048576,4096,64,1) -> Float(4194304,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_100 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_100 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_100 (CudnnConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 Time: 5.60947\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(4194304,4096,64,1), Float(4194304,4096,64,1) -> Float(4194304,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Add_101 (ElementWise)\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 1 Time: 2.34189\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 1 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: ElementWise Tactic: 1\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(4194304,4096,64,1) -> Float(4194304,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: PWN(Relu_102) (PointWiseV2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 Time: 1.56672\n",
      "[01/11/2022-07:43:38] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:38] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:38] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:38] [TRT] [V] *************** Autotuning format combination: Float(4194304,4096,64,1) -> Float(2097152,4096,64,1) ***************\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_103 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_103 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:38] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:38] [TRT] [V] --------------- Timing Runner: Conv_103 (CudnnConvolution)\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 112 Time: 8.9303\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:39] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:39] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:39] [TRT] [V] *************** Autotuning format combination: Float(4194304,4096,64,1) -> Float(8388608,4096,64,1) ***************\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: Conv_108 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:39] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: Conv_108 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:39] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: Conv_108 (CudnnConvolution)\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 112 Time: 37.7068\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:39] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:39] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:39] [TRT] [V] *************** Autotuning format combination: Float(4194304,4096,64,1) -> Float(1048576,4096,64,1) ***************\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: Conv_141 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:39] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: Conv_141 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:39] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: Conv_141 (CudnnConvolution)\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 112 Time: 38.9294\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:39] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:39] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:39] [TRT] [V] *************** Autotuning format combination: Float(2097152,4096,64,1) -> Float(2097152,4096,64,1) ***************\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: PWN(Relu_104) (PointWiseV2)\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 0 Time: 0.785408\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:39] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:39] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:39] [TRT] [V] *************** Autotuning format combination: Float(1048576,4096,64,1) -> Float(1048576,4096,64,1) ***************\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: PWN(Relu_142) (PointWiseV2)\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 0 Time: 0.39424\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:39] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:39] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:39] [TRT] [V] *************** Autotuning format combination: Float(2097152,4096,64,1) -> Float(2097152,4096,64,1) ***************\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: Conv_105 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:39] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: Conv_105 (CudnnConvolution)\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 112 Time: 39.6493\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:39] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:39] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:39] [TRT] [V] *************** Autotuning format combination: Float(1048576,4096,64,1) -> Float(86016,4096,64,1) ***************\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: Conv_143 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:39] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: Conv_143 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:39] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: Conv_143 (CudnnConvolution)\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 112 Time: 0.269312\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:39] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:39] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:39] [TRT] [V] *************** Autotuning format combination: Float(2097152,4096,64,1) -> Float(2097152,4096,64,1) ***************\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: PWN(Relu_106) (PointWiseV2)\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 0 Time: 0.784384\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:39] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:39] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:39] [TRT] [V] *************** Autotuning format combination: Float(2097152,4096,64,1) -> Float(8388608,4096,64,1) ***************\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: Conv_107 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:39] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: Conv_107 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:39] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: Conv_107 (CudnnConvolution)\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 112 Time: 17.4653\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:39] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:39] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:39] [TRT] [V] *************** Autotuning format combination: Float(8388608,4096,64,1), Float(8388608,4096,64,1) -> Float(8388608,4096,64,1) ***************\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: Add_109 (ElementWise)\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 1 Time: 4.6848\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 1 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:39] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: ElementWise Tactic: 1\n",
      "[01/11/2022-07:43:39] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:39] [TRT] [V] *************** Autotuning format combination: Float(8388608,4096,64,1) -> Float(8388608,4096,64,1) ***************\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: PWN(Relu_110) (PointWiseV2)\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 0 Time: 3.13037\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:39] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:39] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:39] [TRT] [V] *************** Autotuning format combination: Float(86016,4096,64,1) -> Float(5505024,262144,512,1) ***************\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: Resize_156 (Resize)\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 1 Time: 1.52781\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 1 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:39] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: Resize Tactic: 1\n",
      "[01/11/2022-07:43:39] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:39] [TRT] [V] *************** Autotuning format combination: Float(8388608,4096,64,1) -> Float(2097152,4096,64,1) ***************\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: Conv_111 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:39] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: Conv_111 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:39] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: Conv_111 (CudnnConvolution)\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 112 Time: 16.6492\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:39] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:39] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:39] [TRT] [V] *************** Autotuning format combination: Float(2097152,4096,64,1) -> Float(2097152,4096,64,1) ***************\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: PWN(Relu_112) (PointWiseV2)\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 0 Time: 0.785408\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:39] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:39] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:39] [TRT] [V] *************** Autotuning format combination: Float(2097152,4096,64,1) -> Float(2097152,4096,64,1) ***************\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: Conv_113 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:39] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: Conv_113 (CudnnConvolution)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 112 Time: 39.6943\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:39] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:39] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:39] [TRT] [V] *************** Autotuning format combination: Float(2097152,4096,64,1) -> Float(2097152,4096,64,1) ***************\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: PWN(Relu_114) (PointWiseV2)\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 0 Time: 0.784384\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:39] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:39] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:39] [TRT] [V] *************** Autotuning format combination: Float(2097152,4096,64,1) -> Float(8388608,4096,64,1) ***************\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: Conv_115 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:39] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: Conv_115 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:39] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: Conv_115 (CudnnConvolution)\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 112 Time: 18.39\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:39] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:39] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:39] [TRT] [V] *************** Autotuning format combination: Float(8388608,4096,64,1), Float(8388608,4096,64,1) -> Float(8388608,4096,64,1) ***************\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: Add_116 (ElementWise)\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 1 Time: 4.66637\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 1 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:39] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: ElementWise Tactic: 1\n",
      "[01/11/2022-07:43:39] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:39] [TRT] [V] *************** Autotuning format combination: Float(8388608,4096,64,1) -> Float(8388608,4096,64,1) ***************\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: PWN(Relu_117) (PointWiseV2)\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 0 Time: 3.1273\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:39] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:39] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:39] [TRT] [V] *************** Autotuning format combination: Float(8388608,4096,64,1) -> Float(2097152,4096,64,1) ***************\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: Conv_118 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:39] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: Conv_118 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:39] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: Conv_118 (CudnnConvolution)\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 112 Time: 16.8152\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:39] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:39] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:39] [TRT] [V] *************** Autotuning format combination: Float(2097152,4096,64,1) -> Float(2097152,4096,64,1) ***************\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: PWN(Relu_119) (PointWiseV2)\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 0 Time: 0.78336\n",
      "[01/11/2022-07:43:39] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:39] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:39] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:39] [TRT] [V] *************** Autotuning format combination: Float(2097152,4096,64,1) -> Float(2097152,4096,64,1) ***************\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: Conv_120 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:39] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:39] [TRT] [V] --------------- Timing Runner: Conv_120 (CudnnConvolution)\n",
      "[01/11/2022-07:43:40] [TRT] [V] Tactic: 112 Time: 39.5131\n",
      "[01/11/2022-07:43:40] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:40] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:40] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:40] [TRT] [V] *************** Autotuning format combination: Float(2097152,4096,64,1) -> Float(2097152,4096,64,1) ***************\n",
      "[01/11/2022-07:43:40] [TRT] [V] --------------- Timing Runner: PWN(Relu_121) (PointWiseV2)\n",
      "[01/11/2022-07:43:40] [TRT] [V] Tactic: 0 Time: 0.787456\n",
      "[01/11/2022-07:43:40] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:40] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:40] [TRT] [V] *************** Autotuning format combination: Float(2097152,4096,64,1) -> Float(8388608,4096,64,1) ***************\n",
      "[01/11/2022-07:43:40] [TRT] [V] --------------- Timing Runner: Conv_122 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:40] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:40] [TRT] [V] --------------- Timing Runner: Conv_122 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:40] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:40] [TRT] [V] --------------- Timing Runner: Conv_122 (CudnnConvolution)\n",
      "[01/11/2022-07:43:40] [TRT] [V] Tactic: 112 Time: 17.3261\n",
      "[01/11/2022-07:43:40] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:40] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:40] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:40] [TRT] [V] *************** Autotuning format combination: Float(8388608,4096,64,1), Float(8388608,4096,64,1) -> Float(8388608,4096,64,1) ***************\n",
      "[01/11/2022-07:43:40] [TRT] [V] --------------- Timing Runner: Add_123 (ElementWise)\n",
      "[01/11/2022-07:43:40] [TRT] [V] Tactic: 1 Time: 5.34957\n",
      "[01/11/2022-07:43:40] [TRT] [V] Tactic: 1 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:40] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: ElementWise Tactic: 1\n",
      "[01/11/2022-07:43:40] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:40] [TRT] [V] *************** Autotuning format combination: Float(8388608,4096,64,1) -> Float(8388608,4096,64,1) ***************\n",
      "[01/11/2022-07:43:40] [TRT] [V] --------------- Timing Runner: PWN(Relu_124) (PointWiseV2)\n",
      "[01/11/2022-07:43:40] [TRT] [V] Tactic: 0 Time: 3.43757\n",
      "[01/11/2022-07:43:40] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:40] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:40] [TRT] [V] *************** Autotuning format combination: Float(8388608,4096,64,1) -> Float(2097152,4096,64,1) ***************\n",
      "[01/11/2022-07:43:40] [TRT] [V] --------------- Timing Runner: Conv_125 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:40] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:40] [TRT] [V] --------------- Timing Runner: Conv_125 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:40] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:40] [TRT] [V] --------------- Timing Runner: Conv_125 (CudnnConvolution)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/11/2022-07:43:40] [TRT] [V] Tactic: 112 Time: 167.572\n",
      "[01/11/2022-07:43:40] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:40] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:40] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:40] [TRT] [V] *************** Autotuning format combination: Float(2097152,4096,64,1) -> Float(2097152,4096,64,1) ***************\n",
      "[01/11/2022-07:43:40] [TRT] [V] --------------- Timing Runner: PWN(Relu_126) (PointWiseV2)\n",
      "[01/11/2022-07:43:40] [TRT] [V] Tactic: 0 Time: 0.786528\n",
      "[01/11/2022-07:43:40] [TRT] [V] Tactic: 0 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:40] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: PointWiseV2 Tactic: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:40] [TRT] [V] *************** Autotuning format combination: Float(2097152,4096,64,1) -> Float(86016,4096,64,1) ***************\n",
      "[01/11/2022-07:43:40] [TRT] [V] --------------- Timing Runner: Conv_127 (FusedConvActConvolution)\n",
      "[01/11/2022-07:43:40] [TRT] [V] FusedConvActConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:40] [TRT] [V] --------------- Timing Runner: Conv_127 (CudaDepthwiseConvolution)\n",
      "[01/11/2022-07:43:40] [TRT] [V] CudaDepthwiseConvolution has no valid tactics for this config, skipping\n",
      "[01/11/2022-07:43:40] [TRT] [V] --------------- Timing Runner: Conv_127 (CudnnConvolution)\n",
      "[01/11/2022-07:43:40] [TRT] [V] Tactic: 112 Time: 0.463872\n",
      "[01/11/2022-07:43:40] [TRT] [V] Tactic: 112 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:40] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: CudnnConvolution Tactic: 112\n",
      "[01/11/2022-07:43:40] [TRT] [V] =============== Computing costs for \n",
      "[01/11/2022-07:43:40] [TRT] [V] *************** Autotuning format combination: Float(86016,4096,64,1) -> Float(5505024,262144,512,1) ***************\n",
      "[01/11/2022-07:43:40] [TRT] [V] --------------- Timing Runner: Resize_140 (Resize)\n",
      "[01/11/2022-07:43:40] [TRT] [V] Tactic: 1 Time: 1.7152\n",
      "[01/11/2022-07:43:40] [TRT] [V] Tactic: 1 A valid tactic is found. Rest of the tactics are skipped.\n",
      "[01/11/2022-07:43:40] [TRT] [V] >>>>>>>>>>>>>>> Chose Runner Type: Resize Tactic: 1\n",
      "[01/11/2022-07:43:40] [TRT] [V] Formats and tactics selection completed in 3.66211 seconds.\n",
      "[01/11/2022-07:43:40] [TRT] [V] After reformat layers: 127 layers\n",
      "[01/11/2022-07:43:40] [TRT] [V] Pre-optimized block assignment.\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 268435456\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 268435456\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 67108864\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 67108864\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 268435456\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 268435456\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 268435456\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 268435456\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 67108864\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 67108864\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 67108864\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 67108864\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 268435456\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 268435456\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 268435456\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 67108864\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 67108864\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 67108864\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 67108864\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 268435456\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 268435456\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 268435456\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 67108864\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 67108864\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 67108864\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 67108864\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 268435456\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 268435456\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 268435456\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 536870912\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 268435456\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 268435456\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 268435456\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 268435456\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 1073741824\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 1073741824\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 1073741824\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 1073741824\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 268435456\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 268435456\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 268435456\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 268435456\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 1073741824\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 1073741824\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 1073741824\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 268435456\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 268435456\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 268435456\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 268435456\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 1073741824\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 1073741824\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 1073741824\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 268435456\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 268435456\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 11010048\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 11010048\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 1073741824\n",
      "[01/11/2022-07:43:40] [TRT] [V] Total Activation Memory: 42434822144\n",
      "[01/11/2022-07:43:40] [TRT] [I] Detected 1 inputs and 2 output network tensors.\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_6 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_7) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: MaxPool_8 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_9 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_14 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_10) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_11 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_12) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_13 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Add_15 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_16) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_17 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_18) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_19 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_20) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_21 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Add_22 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_23) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_24 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_25) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_26 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_27) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_28 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Add_29 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_30) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_31 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_36 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_32) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_33 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_34) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_35 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Add_37 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_38) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_39 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_40) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_41 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_42) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_43 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Add_44 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_45) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_46 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_47) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_48 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_49) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_50 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Add_51 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_52) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_53 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_54) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_55 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_56) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_57 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Add_58 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_59) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_60 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_65 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_61) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_62 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_63) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_64 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Add_66 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_67) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_68 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_69) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_70 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_71) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_72 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Add_73 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_74) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_75 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_76) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_77 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_78) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_79 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Add_80 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_81) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_82 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_83) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_84 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_85) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_86 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Add_87 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_88) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_89 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_90) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_91 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_92) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_93 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Add_94 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_95) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_96 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_97) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_98 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_99) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_100 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Add_101 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_102) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_103 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_108 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_141 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_104) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_142) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_105 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_143 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_106) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_107 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Add_109 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_110) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Resize_156 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_111 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_112) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_113 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_114) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_115 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Add_116 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_117) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_118 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_119) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_120 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_121) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_122 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Add_123 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_124) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_125 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: PWN(Relu_126) Host Persistent: 244 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Conv_127 Host Persistent: 32 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [V] Layer: Resize_140 Host Persistent: 0 Device Persistent: 0 Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [I] Total Host Persistent Memory: 14880\n",
      "[01/11/2022-07:43:40] [TRT] [I] Total Device Persistent Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [I] Total Scratch Memory: 0\n",
      "[01/11/2022-07:43:40] [TRT] [I] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 0 MiB, GPU 4512 MiB\n",
      "[01/11/2022-07:43:40] [TRT] [I] [BlockAssignment] Algorithm ShiftNTopDown took 5.98117ms to assign 5 blocks to 125 nodes requiring 3489660928 bytes.\n",
      "[01/11/2022-07:43:40] [TRT] [V] Optimized block assignment.\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 1073741824\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 1073741824\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 1073741824\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [V] Block size 134217728\n",
      "[01/11/2022-07:43:40] [TRT] [I] Total Activation Memory: 3489660928\n",
      "[01/11/2022-07:43:40] [TRT] [V] Using cublasLt as a tactic source\n",
      "[01/11/2022-07:43:40] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 4428, GPU 6910 (MiB)\n",
      "[01/11/2022-07:43:40] [TRT] [V] Using cuDNN as a tactic source\n",
      "[01/11/2022-07:43:40] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 4428, GPU 6918 (MiB)\n"
     ]
    }
   ],
   "source": [
    "ONNX_PATH = \"fcn_resnet50.onnx\"\n",
    "calib_data_path = 'calibration/'\n",
    "\n",
    "# Now we create a calibrator and give it the location of our calibration data.\n",
    "# We also allow it to cache calibration data for faster engine building.\n",
    "calibration_cache = \"mnist_calibration.cache\"\n",
    "calib = EntropyCalibrator(calib_data_path, total_images=300, batch_size=1, cache_file=calibration_cache)\n",
    "    \n",
    "TRT_LOGGER = trt.Logger(trt.Logger.VERBOSE)       \n",
    "batch_size = 1 # This is inference batch size that can be different from calibration batch size.\n",
    "with build_int8_engine(ONNX_PATH, calib, batch_size, calibration_cache) as engine, engine.create_execution_context() as context:                    \n",
    "    context.active_optimization_profile = 0  # For dynamic shapes\n",
    "    context.set_binding_shape(0, (batch_size, 3, 32, 32))  # For dynamic shapes\n",
    "        \n",
    "#     # Batch size for inference can be different than batch size used for calibration.\n",
    "#     test_set, test_labels = load_cifar_data(cifar10_data_path)\n",
    "#     check_accuracy(context, batch_size, test_set=test_set, test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6556f1d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7476ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f68fd79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae35db49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a20d94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "commond = 'trtexec' + ' --onnx=' + onnx_model_name + ' --shapes=input0:1x3x512x512' + ' --int8'\n",
    "cli_output = os.popen(commond).read()\n",
    "print(cli_output)\n",
    "# !trtexec --onnx=onnx_model_name --shapes=input0:1x3x512x512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938181aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de078378",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
